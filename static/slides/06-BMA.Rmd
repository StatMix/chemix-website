---
title: "Tutorial 06 - Bayesian Model Averaging"
author: "Olivia Fan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    css: "./tutorials.css"
    toc: true
    toc_float: true
---

<!-- Setup -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning=FALSE,
                      message=FALSE)
```

## Topics

-   Basic ideas of BMA

-   Application of BMA using R

-   Estimation and Interpretation

-   Advantages of BMA over all-or-none model selection

## Libraries

The R package `BMA` provides ways of carrying out BMA for linear regression, generalized linear models, and survival or event history analysis using Cox proportional hazards models. It contains functions for plotting the BMA posterior distributions of the model parameters, as well as an image plot function that provides a way of visualizing the BMA output. The functions `bicreg` and `bic.glm` provide fast and automatic default ways of doing this for the model classes considered.

```{r}
library(BMA)
library(tidyverse)
library(broom.mixed)
```

To illustrate how BMA takes account of model uncertainty about the variables to be included in linear regression, we will be using the `nhanes` dataset where the variables are described in the file `nhanes-codebook.txt`. Load this data with the `load` function and specify the data file.

```{r}
load(file='nhanes1518.rda')
```

## Exploratory Data Analysis

We first explore the URX predictors (i.e. the ones related to phthalates concentrations), subsetting subset the dataset to include only the predictors in the weighted quantile sum module, and then filter out `NA` values:

```{r}
nhanes_URX<-nhanes1518%>%
  select(BMXBMI, URXUCR, URXCNP,URXCOP,URXECP,URXHIBP,URXMBP,URXMC1,URXMEP,URXMHBP,URXMHH)%>%
  na.omit()
```

We first start exploring the data by plotting a correlation matrix between the variables of interest. We can use the `corrplot` function within the `corrplot()` package, and adjust the style to fit our aesthetics of desire. We see that the predictors are highly correlated, especially between URXMHH and URXECP, and URXMHBP and URXMBP.

```{r}
library(corrplot)
corr_mat=cor(nhanes_URX,method="s")
corrplot(corr_mat, 
         addCoef.col = 1,    
         number.cex = 0.5) # Change font size of correlation coefficients
# other styles:
# default 
# corrplot(corr_mat) # circles
# squares, variables presented in an alphabet order
# corrplot(corr_mat, method = 'color', order = 'alphabet') # squares
# can choose different style for lower and upper triangle, can order the variable by clustering result
#corrplot.mixed(corr_mat, lower = 'shade', upper = 'pie', order = 'hclust')
```

## Bayesian Model Averaging

> Motivated from chemical mixtures, one way to deal with highly correlated predictors is to select variable, or select model. blah blah Traditionally, analysis often proceeds by first selecting the best model according to some criterion and then learning about the parameters given that the selected model is the underlying truth.potential issues: cannot quantify model uncertainty. + "There are often many modeling choices that are secondary to the main questions of interest but can still have an important effect on conclusions." An alternative is to learn the parameters for *all* candidate models and then combine the estimates according to the posterior probabilities of associated model. *Bayesian Model Averaging*(BMA) carry this model combination idea. Specifically, BMA xxxx (rephrase this "A parameter estimate (or a prediction of new observations) obtained by averaging the estimates (or predictions) of the different models under consideration, each weighted by its model probability".) The math formula below illustrate xxx.

> Provide the math formula here.

> Explain the math formula and say we consider Mk to be linear regression and logistic regression as examples. 


In this module, we will investigate this method known as *Bayesian Model Averaging*, and the advantages it has over all-or-none model selection.


## An Example: Linear Regression

> write down the math formula for BMA here (linear regression model)

We fit a BMA model on the dataset using BMI as the response variable, and all of the 10 URX variables as predictor variables, after log transforming both variables: 

> explain why you do the log transformation: long tail/extreme values in the original scale)

> explain what the other arguments do

```{r}
x.nhanes <-log(nhanes_URX[,-1])
y.nhanes <-log(nhanes_URX$BMXBMI)
?bicreg
nhanes.bicreg <- bicreg(x=x.nhanes, y=y.nhanes)
summary(nhanes.bicreg)
```

> We need to provide example on how to dig insights using the model instead of purely presenting the R coding. Read the Section 7.2 Example 2 of Bayesian Model Averaging: A Tutorial carefully. Please also read the application part of this paper carefully: [Bayesian Model Averaging: Theoretical Developments and Practical Applications](https://www.cambridge.org/core/journals/political-analysis/article/bayesian-model-averaging-theoretical-developments-and-practical-applications/3179D92A3C9353DE7E4674987C33FD28)

> Explain the output of `summary()`. What does each column represent? How does r2, BIC, post prob help us to specify the important variables or the model? Also comment on what valuable information we obtain. For example, what are the important variables? What questions it help us answer?

In R, the `imageplot` function may be used to create an image of the model space that looks like a crossword puzzle:

```{r}
imageplot.bma(nhanes.bicreg)
```

Here, the predictors, including the intercept, are on the y-axis, while the x-axis corresponds to each different model. The red color indicates when the variable estimate is positive, the blue color indicates when the variable estimate is negative, and the beige color is the color to use when the variable is not included in the model. Here the imageplot indicates that most of the URX variables are significant in predicting the BMI level, except for `URXMC1` which is not included in all of the six models produced by BMA.

If we view the image by rows, we can see whether one variable is included in a particular model. For each variable, there are only 6 models in which it will appear. For example, we see that `URXUCR` and `URXMEP` appear in all the models. `URXMHH` appears in the top 2 models with larger posterior probabilities, but not the last 4 models.

We can also plot the posterior distributions of these coefficients to take a closer look at the distributions:

```{r}
plot (nhanes.bicreg,mfrow=c(3,3))
```

This plot agrees with the summary table we obtained above, which shows that the posterior probability distributions of `URXMC1` and `URXCOP` have a very large point mass at 0, while the distribution of `URXUCR` and `URXMEP` have relatively small mass at 0. There is a slighly little tip at 0 for the variable `URXUCR`, indicating that the posterior inclusion probability of `URXECP` is not exactly 1. However, since the probability mass for `URXUCR` to be 0 is so small, that we are almost certain that `URXECP` should be included under Bayesian model averaging.

## An Example: Logistic Regression

Now we illustrate BMA for logistic regression using the URX variables as predictors on the response BMI

We explore the BMI value as a categorical between 1 (denoting high BMI value) and 0 (denoting low BMI value), with BMI value of 22 as the threshold:

```{r}
BMI_category <- nhanes_URX %>%
  mutate(BMI_Level = case_when(BMXBMI<22 ~ 0, BMXBMI >=22 ~ 1))
```

Then we fit the Bayesian logistic regression model on predictors including categorical variable `BMI_Level`:

```{r}
nhanes.bic.glm<-bic.glm(BMI_Level~.-BMXBMI -BMI_Level, data=BMI_category, glm.family="binomial")
summary(nhanes.bic.glm, conditional=T, digits=3)
```

And then we graph the posterior distributions of the fitted model, as well as the image plot summarizing the BMA analysis for logistic regression of the BMI value.

Here the imageplot indicates that most of the URX variables are significant in predicting the BMI level, except for `URXCOP` and `URXMEP` which is not included in all of the four models produced by BMA.

```{r}
imageplot.bma(nhanes.bic.glm)
```

We can also plot the posterior distributions of these coefficients to take a closer look at the distributions:

```{r}
plot (nhanes.bic.glm,mfrow=c(3,3))
```

This plot agrees with the summary table we obtained above, which shows that the posterior probability distributions of`URXCOP` and `URXMEP` have a very large point mass at 0, while the distribution of `URXUCR` and `URXMHBP` have relatively small mass at 0. There is a slighly little tip at 0 for the variable `URXMHBP`, indicating that the posterior inclusion probability of `URXMHBP` is not exactly 1. However, since the probability mass for `URXUCR` to be 0 is so small, that we are almost certain that `URXMHBP` should be included under Bayesian model averaging.

## Bayesian Model Diagnostics

```{r}
# library(coda)
# library(bayesplot)
# autocorr(nhanes.bic.glm)
```

## Bayesian Model Predictions

With the `predict` function, we can understand uncertainty besides classification, taking a look at the predictive probabilities for the two class classification problem:

```{r}
predicted<-predict(nhanes.bic.glm, newdata=BMI_category)
predicted
```

## 7. BMA vs. WQS

## 8. Why Bayesian Model Averaging? Advantages

### Model Uncertainty

-   It is important to take account of model uncertainty about statistical structure when making inferences. Oftentimes, there is remaining uncertainty not only about parameters, but also about the underlying true model. In this case, a Bayesian analysis allows one to take into account not only uncertainty about the parameters given a particular model, but also uncertainty across all models combined.

### Simultaneous Scenarios

-   Allows users to incorporate several competing models in the estimation process. In theory, BMA provides better average predictive performance than any single model that could be selected. BMA avoids the all-or-nothing mentality that is associated with classical hypothesis testing, in which a model is either accepted or rejected wholesale. In contrast, BMA retains all model uncertainty until the final inference stage, which may or may not feature a discrete decision.

### Model Misspecification

-   BMA is relatively robust to model misspecification. If one does select a single model, then one had better be sure of being correct. With BMA, a range of rival models contribute to estimates and predictions, and chances are that one of the models in the set is at least approximately correct.

> Add references
