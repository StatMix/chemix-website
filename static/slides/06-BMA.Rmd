---
title: "Tutorial 06 - Bayesian Model Averaging"
author: "Olivia Fan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    css: "./tutorials.css"
    toc: true
    toc_float: true
---

<!-- Setup -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning=FALSE,
                      message=FALSE)
```

## Background & Introduction 

Traditionally, analysis often proceeds by first selecting the best model according to some criterion and then learning about the parameters given that the selected model is the underlying truth. However, from the Bayesian perspective, an alternative is to learn the parameters for *all* candidate models and then combine the estimates according to the posterior probabilities of associated model. In this module, we will investigate this method known as *Bayesian Model Averaging*, and the advantages it has over all-or-none model selection. 

## Topics 

- Advantages of BMA over all-or-none model selection through the examples of analysis of covariance, meta-analysis and network analysis.

- Estimate the slope and intercept of the regression line using the least squares method.

- Interpret the slope and intercept of the regression line.

## 1. Definitions

### Bayes factor
The change from prior to posterior model odds brought about by the data; equivalently, how much more likely the observed data are under one model versus another.

### Bayesian model average
A parameter estimate (or a prediction of new observations) obtained by averaging the estimates (or predictions) of the different models under consideration, each weighted by its model probability.

## 2. Libraries

The R package `BMA` provides ways of carrying out BMA for linear regression, generalized linear models, and survival or event history analysis using Cox proportional hazards models.
The library contains functions for plotting the BMA posterior distributions of the model parameters, as well as an image plot function that provides a way of visualizing the BMA output. The functions bicreg and bic.glm  provide fast and automatic default ways of doing this for the model classes considered. 

```{r}
library(BMA)
library(tidyverse)
library(broom.mixed)
```

## 3. Exploratory Data Analysis

To illustrate how BMA takes account of model uncertainty about the variables to be included in linear regression, we will be using the ``nhanes`` dataset where the variables are described in the file `nhanes-codebook.txt`. Load this data with the `load` function and specify the data file. 

```{r}
load(file='nhanes1518.rda')
```

We first explore the URX predictors (i.e. the ones related to phthalates concentrations), subsetting subset the dataset to include only the predictors in the weighted quantile sum module, and then filter out `NA` values:

```{r}
nhanes_URX<-nhanes1518%>%
  select(BMXBMI, URXUCR, URXCNP,URXCOP,URXECP,URXHIBP,URXMBP,URXMC1,URXMEP,URXMHBP,URXMHH)%>%na.omit()
```

```{r}
library(corrplot)
corr_mat=cor(nhanes_URX,method="s")
corrplot(corr_mat) # circles
corrplot(corr_mat, method = 'color', order = 'alphabet') # squares
corrplot(corr_mat, # colorful number
         addCoef.col = 1,    # Change font size of correlation coefficients
         number.cex = 0.5) 
corrplot.mixed(corr_mat, lower = 'shade', upper = 'pie', order = 'hclust')
```

We first start exploring the data by plotting a correlation matrix between the variables of interest. We can use the `corrplot` function within the `corrplot()` package, and adjust the style to fit our aesthetics of desire. We see that the predictors are highly correlated, especially between URXMHH and URXECP, and URXMHBP and URXMBP.  

## 3. Example : Linear Regression

We fit a BMA model on the dataset using BMI as the response variable, and all of the 10 URX  variables as predictor variables, after log transforming both variables:

```{r}
x.nhanes <-log(nhanes_URX[-1])
y.nhanes <-log(nhanes_URX[1])
y.nhanes<-as.numeric(unlist(y.nhanes))
nhanes.bicreg <- bicreg(x.nhanes, y.nhanes)
summary(nhanes.bicreg)
```

In R, the `imageplot` function may be used to create an image of the model space that looks like a crossword puzzle:

```{r}
imageplot.bma(nhanes.bicreg)
```
Here, the predictors, including the intercept, are on the y-axis, while the x-axis corresponds to each different model. The red color indicates when the variable estimate is positive, the blue color indicates when the variable estimate is negative, and the beige color is the color to use when the variable is not included in the model. Here the imageplot indicates that most of the URX variables are significant in predicting the BMI level, except for `URXMC1` which is not included in all of the six models produced by BMA.

If we view the image by rows, we can see whether one variable is included in a particular model. For each variable, there are only 6 models in which it will appear. For example, we see that `URXUCR` and `URXMEP` appear in all the models. `URXMHH` appears in the top 2 models with larger posterior probabilities, but not the last 4 models.

We can also plot the posterior distributions of these coefficients to take a closer look at the distributions:

```{r}
plot (nhanes.bicreg,mfrow=c(3,3))
```


This plot agrees with the summary table we obtained above, which shows that the posterior probability distributions of `URXMC1` and `URXCOP` have a very large point mass at 0, while the distribution of `URXUCR` and `URXMEP` have relatively small mass at 0. There is a slighly little tip at 0 for the variable `URXUCR`, indicating that the posterior inclusion probability of `URXECP` is not exactly 1. However, since the probability mass for `URXUCR` to be 0 is so small, that we are almost certain that `URXECP` should be included under Bayesian model averaging.


## 4 Example: Logistic Regression

Now we illustrate BMA for logistic regression using the URX variables as predictors on the response BMI

We explore the BMI value as a categorical between 1 (denoting high BMI value) and 0 (denoting low BMI value), with BMI value of 22 as the threshold:

```{r}
BMI_category <- nhanes_URX %>%
  mutate(BMI_Level = case_when(BMXBMI<22 ~ 0, BMXBMI >=22 ~ 1))
```

Then we fit the Bayesian logistic regression model on predictors including categorical variable `BMI_Level`:

```{r}
nhanes.bic.glm<-bic.glm(BMI_Level~.-BMXBMI -BMI_Level, data=BMI_category, glm.family="binomial")
summary(nhanes.bic.glm, conditional=T, digits=3)
```

And then we graph the posterior distributions of the fitted model, as well as the image plot summarizing the BMA analysis for logistic regression of the BMI value.

Here the imageplot indicates that most of the URX variables are significant in predicting the BMI level, except for `URXCOP` and `URXMEP` which is not included in all of the four models produced by BMA.

```{r}
imageplot.bma(nhanes.bic.glm)
```
We can also plot the posterior distributions of these coefficients to take a closer look at the distributions:

```{r}
plot (nhanes.bic.glm,mfrow=c(3,3))
```

This plot agrees with the summary table we obtained above, which shows that the posterior probability distributions of`URXCOP` and `URXMEP`  have a very large point mass at 0, while the distribution of `URXUCR` and `URXMHBP` have relatively small mass at 0. There is a slighly little tip at 0 for the variable `URXMHBP`, indicating that the posterior inclusion probability of `URXMHBP` is not exactly 1. However, since the probability mass for `URXUCR` to be 0 is so small, that we are almost certain that `URXMHBP` should be included under Bayesian model averaging.


## 5. Bayesian Model Diagnostics


```{r}
# library(coda)
# library(bayesplot)
# autocorr(nhanes.bic.glm)
```

## 6. Bayesian Model Predictions

With the `predict` function, we can understand uncertainty besides classification, taking a look at the predictive probabilities for the two class classification problem:

```{r}
predicted<-predict(nhanes.bic.glm, newdata=BMI_category)
predicted
```

## 7. BMA vs. WQS

## 8. Why Bayesian Model Averaging? Advantages

### Model Uncertainty
- It is important to take account of model uncertainty about statistical structure when making inferences. Oftentimes, there is remaining uncertainty not only about parameters, but also about the underlying true model. In this case, a Bayesian analysis allows one to take into account not only uncertainty about the parameters given a particular model, but also uncertainty across all models combined.

### Simultaneous Scenarios
- Allows users to incorporate several competing models in the estimation process. In theory, BMA provides better average predictive performance than any single model that could be selected. BMA avoids the all-or-nothing mentality that is associated with classical hypothesis testing, in which a model is either accepted or rejected wholesale. In contrast, BMA retains all model uncertainty until the final inference stage, which may or may not feature a discrete decision.

### Model Misspecification
- BMA is relatively robust to model misspecification. If one does select a single model, then one had better be sure of being correct. With BMA, a range of rival models contribute to estimates and predictions, and chances are that one of the models in the set is at least approximately correct.


## Related Topics

| **Topics** | **Concepts** |
|-------------------------------------------------------------|:--:|
| ANOVA | Null hypothesis |
| Quantile factoring | use qualitative  as levels |
| Cross-validation 	|  k-fold, LOOCV	|
| Model section 	|  AIC, BIC, Cp	|