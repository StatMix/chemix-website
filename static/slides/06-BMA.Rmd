---
title: "Bayesian Model Averaging"
author: "Olivia Fan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    css: "./tutorials.css"
    toc: true
    toc_float: true
---

<!-- Setup -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning=FALSE,
                      message=FALSE)
```

## Topics

-   Basic ideas of Bayesian Model Averaging

-   Application of Bayesian Model Averaging using R package BAS

-   Estimation, Interpretation and Prediction

-   Advantages of Bayesian Model Averaging over all-or-none model selection

## Bayesian Model Averaging

Suppose we are interested in exploring the relationship between the chemical exposures and the outcome (BMI for example). These chemical exposures are usually highly correlated with each other (known as **multicollinearity**). One way is to summarize these highly correlated chemical mixtures and explore the potential influence of these chemical mixtures as a whole (See tutorials of [weighted quantile sum regression](https://chem-mix.netlify.app/slides/03-wqs), [Bayesian weighted sum regression](https://chem-mix.netlify.app/slides/04-bayesian-weighted-sums), [quantile g-computation](https://chem-mix.netlify.app/slides/05-qgcomp.html)). However, the model require a strict assumption that all exposures should contribute to the same direction which is not applicable in some situations. Another popular way is to select a subset of these chemical mixtures as representatives, which is called variable selection, or model selection. Traditionally, analysis often proceeds by first selecting the best model according to some criterion and then learning about the parameters given that the selected model is the underlying truth. However, this approach has potential issues: (1) We cannot quantify model uncertainty through these all-or-none selection methods. (2) There are often many modeling choices that are secondary to the main questions of interest but can still have an important effect on conclusions. 

As an alternative, **Bayesian Model Averaging** (BMA) carry a model combination idea. Instead of choosing only one model, it learns the parameters for **all** candidate models and then combine the estimates according to the uncertainty (posterior probabilities) of associated model. Specifically, this is done through a parameter estimate obtained by averaging the predictions of the different models under consideration, each weighted by its model probability. 

Given quantity of interest $\Delta$ (which can be a future observation $Y^*$, or a parameter of interest $\beta_j$), then its posterior distribution of given data $D$ is 

$$Pr(\Delta|D)=\sum_{k=1}^KPr(\Delta|M_k,D)Pr(M_k|D).$$
Let $K$ denote the number of all potential models, the model probability of model $k$ ($Pr(M_k|D)$) is calculated by

$$Pr(M_k|D) = \frac{Pr(D|M_k)Pr(M_k)}{\sum_{l=1}^K Pr(D|M_l)Pr(M_l)}$$

The model $M$ can take various forms, such as linear models, generalized linear models, survival analysis. Here in this tutorial, we will only focus on BMA for linear regression.

There are three packages available: `BAS`, `BMS`, and `BMA`. A thorough comparison are presented in this [paper](https://www.researchgate.net/profile/Christopher-Parmeter/publication/268216833_Bayesian_model_averaging_in_R/links/54ac4c040cf2479c2ee7b14e/Bayesian-model-averaging-in-R.pdf). Here we use `BAS` package for its fast computation, flexible choices of priors, and various options for inference and diagnosis.


## Libraries

The R package `BAS` provides ways of carrying out Bayesian Model Averaging (BMA) for linear regression, generalized linear models, and survival or event history analysis using Cox proportional hazards models. It contains functions for plotting the BMA posterior distributions of the model parameters, as well as an image plot function that provides a way of visualizing the BMA output. The functions `bas.lm` provide fast and automatic default ways of doing this for the model classes considered.

```{r}
library(BAS)
library(tidyverse)
```

To illustrate how BMA takes account of model uncertainty about the variables to be included in linear regression, we will be using the `nhanes` dataset where the variables are described in the file `nhanes-codebook.txt`. Load this data with the `load` function and specify the data file.

```{r}
load(file='nhanes1518.rda')
```

## Exploratory Data Analysis

We focus on exploring the poential effect of URX predictors (i.e. the ones related to phthalates concentrations) on BMI. Here we first subset the dataset and then filter out `NA` values:

```{r}
nhanes_URX<-nhanes1518%>%
  select(BMXBMI, URXUCR, URXCNP,URXCOP,URXECP,URXHIBP,URXMBP,URXMC1,URXMEP,URXMHBP,URXMHH)%>%
  na.omit()
```

We start exploration by plotting a correlation matrix between the variables of interest. We can use the `corrplot` function within the `corrplot` package, and adjust the style to fit our aesthetics of desire. We see that the predictors are highly correlated, especially between URXMHH and URXECP, and URXMHBP and URXMBP.

```{r}
library(corrplot)
corr_mat=cor(nhanes_URX,method="s")
corrplot(corr_mat, 
         addCoef.col = 1,    
         number.cex = 0.5) # Change font size of correlation coefficients
# other styles:
# default 
# corrplot(corr_mat) # circles
# squares, variables presented in an alphabet order
# corrplot(corr_mat, method = 'color', order = 'alphabet') # squares
# can choose different style for lower and upper triangle, can order the variable by clustering result
#corrplot.mixed(corr_mat, lower = 'shade', upper = 'pie', order = 'hclust')
```

```{r}

df_long <- nhanes_URX %>% pivot_longer(everything())

# Plot histograms using ggplot
ggplot(df_long, aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ name, scales = "free")
```

We also plot the distributions of our variables of interest involved in the model, and notice that the variables have a long tail in the original scale, which hints at log transformation which we will perform later on.


## Bayesian Model Averaging for Linear Regression Models

As shown in the formula above, BMA addresses model uncertainty in a regression problem, and here we consider $M_k$ to be a linear regression. Suppose a linear model structure, with $y$ being the dependent variable, $\alpha_\gamma$, an intercept, $\beta_\gamma$ the coefficients, and $\epsilon$ a normal IID error term with variance $\sigma^2$:
$$y=\alpha_\gamma+X_\gamma \beta_\gamma+\epsilon \hspace{1cm}\epsilon \sim N(0, \sigma^2I)$$

The value of $\gamma$ determines how much weight or importance is assigned to each individual model in the model set when combining their predictions or estimating parameters.
- When $\gamma = 0$: No model averaging takes place. In this case, only the "best" model (the one with the highest posterior probability) is used for estimation or prediction.
- When $\gamma = 1$: Complete model averaging is performed. All models in the model set are given equal weight, and their predictions or parameter estimates are combined.

In BMA, the inclusion probability ($\pi_i$) for a model or variable i is calculated as the sum of the posterior probabilities of all models that include that model or variable.

We perform log transformation to all the variables since the EDA suggests the long tail and extreme values in the original scale. We fit a BMA model on the dataset using BMI as the response variable, and all of the 10 URX variables as predictor variables. 

To get started, we will use `BAS` with the Zellner-Siow Cauchy prior on the coefficients.

```{r}
nhanes_URX<-log(nhanes_URX)
model <-bas.lm(BMXBMI ~ .,
  data = nhanes_URX,
  prior = "ZS-null",
  modelprior = uniform(), initprobs = "eplogp", method="MCMC",
  force.heredity = FALSE, pivot = TRUE
)
```

Here we specify the `ZS-null` as the prior argument, and the argument `uniform()` means that the prior distribution over the models is a uniform distribution which assigns equal probabilities to all models. The optional argument `initprobs=eplogp` provides a way to initialize the sampling algorithm and order the variables in the tree structure that represents the model space in BAS. The `eplogp` option uses the Bayes factor calibration of p-values $-eplog(p)$ to provide an approximation to the marginal inclusion probability that the coefficient of each predictor is zero, using the p-values from the full model. The `force.heredity = TRUE` argument means that we force levels of a factor to enter or leave together. Pivot is logical variable to allow pivoting of columns when obtaining the OLS estimates of a model so that models that are not full rank can be fit.

```{r}
summary(model)
```


`summary()` provides summary statistics and lists the top 5 models (in a descending order of posterior probability) with the zero-one indicators for variable inclusion. The first column presents posterior probability of inclusion (PIP) for each variable. Here we see that the `URXMEP` is included in every single model (inclusion probability equal to 1), thus deemed to be important by the BMA model. `URXUCR `, `URXCNP`, `URXECP`, `URXHIBP`, `URXMBP`, `URXMHBP`, `URXMHH` has pip greater than or roughly equal to 0.9, suggesting strong correlations with BMI. On the other hand, the inclusion probability of `URXMC1` variable is only 0.06, therefore deemed to be a weak predictor of BMI by the BMA model. The other column represents a binary outcome of whether the model includes the variable or not. The other rows in the summary table are the Bayes factor of each model to the highest probability model (hence its Bayes factor is 1), the posterior probabilities of the models (represented by `PostProbs`), the ordinary $R^2$ of the models, the dimension of the models as well as the log marginal likelihood under the selected prior distribution which are measurements to evaluate performance of the model.

The summary provides a list of the top 5 models (in terms of posterior probability) with the zero-one indicators for variable inclusion. The other columns in the summary are the Bayes factor of each model to the highest probability model (hence its Bayes factor is 1), the posterior probabilities of the models, the ordinary $R^2$ of the models, the dimension of the models (number of coefficients including the intercept) and the log marginal likelihood under the selected prior distribution. 

Bayes Factor is a measure of the relative evidence for two competing models, typically referred to as Model 1 ($M_1$) and Model 2 ($M_2$). It quantifies how much more or less likely the data are under one model compared to the other. If $BF(M_1,M_2) > 1$: It suggests that Model 1 is more supported by the data than Model 2. If $BF(M_1,M_2) < 1$: It suggests that Model 2 is more supported by the data than Model 1.

The `image` function help to visualize the potential model candidates, which looks like a crossword puzzle:

```{r}
image(model, rotate=F)
```

Here, the predictors, including the intercept, are on the y-axis, while the x-axis corresponds to each different model. The red color indicates when the variable estimate is positive, the blue color indicates when the variable estimate is negative, and the beige color is the color to use when the variable is not included in the model. Here the imageplot indicates that most of the URX variables are significant in predicting the BMI level, except for `URXMC1` which is not included in all of the six models produced by BMA.

If we view the image by rows, we can see whether one variable is included in a particular model. For each variable, there are only 6 models in which it will appear. For example, we see that `URXUCR` and `URXMEP` appear in all the models. `URXMHH` appears in the top 2 models with larger posterior probabilities, but not the last 4 models.

We can also plot the posterior distributions of these coefficients to take a closer look at the distributions:

```{r}
coef.model <- coef(model)
par(mfrow=c(2,2))
plot(coef.model, ask = F)
```
This plot agrees with the summary table we obtained above, which shows that the posterior probability distributions of `URXMC1` and `URXCOP` have a very large point mass at 0, while the distribution of `URXUCR` and `URXMEP` have relatively small mass at 0. There is a slighly little tip at 0 for the variable `URXUCR`, indicating that the posterior inclusion probability of `URXECP` is not exactly 1. However, since the probability mass for `URXUCR` to be 0 is so small, that we are almost certain that `URXECP` should be included under Bayesian model averaging. The sticks here represent the inclusion probabilities; if not present, then the BMA model suggests that we should not include the variable.

`confint` provides the credible interval of coefficients, which can be visualized using `plot`.

```{r}
confint(coef.model)
plot(confint(coef.model),estimator = "HPM") 
```
We see that URXCOP does not have significant effect on BMI since the credible interval include 0. URXUCR and URXMEP have strong positive effects on BMI since the credible interval is entirely positive, whereas URXHIBP and URXCNP have negative effects on BMI since the credible interval is negative. `HPM` stands for the highest probability model. Other choices of estimator include MPM (the median probability model of Berbieri and Berger), BPM (best predictive model) and BMA. Beta stands for the posterior mean.

## Model Diagnostics

```{r}
par(mfrow=c(2,2))
plot(model, ask = F)
```

```{r}
diagnostics(model, type = "pip", pch = 16) 
```


## Model Predictions of BMA

`BAS` has methods defined to return fitted values, namely `fitted`, using the observed design matrix and predictions at either the observed data or potentially new values, predict, as with linear regressions. `predict` provides model fitting and prediction. `fitted` produce the fitted values, which is equvalent to predict()$fit


```{r}
BMA <- predict(model, estimator = "BMA")
# predict has additional slots for fitted values under BMA, predictions under each model
BMA_fitted <- fitted(model, estimator = "BMA")
all(BMA_fitted==BMA$fit)
```


## Model Predictions with Model Selection

In addition to using BMA, we can use the posterior means under model selection. This corresponds to a decision rule that combines estimation and selection. `BAS` currently implements the following options:

### Highest Probability Model

Pros:
- Simplicity: HPM is straightforward to understand and implement. It selects the model with the highest posterior probability.
- Interpretable: The HPM is easy to interpret because it directly represents the most probable model.

Cons:
- Overfitting: HPM can be prone to overfitting because it chooses the model that fits the data best, even if it's overly complex.
- Ignores model uncertainty: It doesn't consider the uncertainty associated with other models, potentially leading to suboptimal decisions when multiple models are equally plausible.

```{r}
HPM <- predict(model, estimator = "HPM")

# show the indices of variables in the best model where 0 is the intercept
HPM$bestmodel
```

Now we explore a little more interpretable version with names:

```{r}
variable.names(HPM)
```

### Median Probability Model

```{r}
MPM <- predict(model, estimator = "MPM")
variable.names(MPM)
```


This is the model where all predictors have an inclusion probability greater than or equal to 0.5. This coincides with the HPM if the predictors are all mutually orthogonal, and in this case is the best predictive model under squared error loss.

Pros:
- Reduces sensitivity to outliers: MPM is less affected by extreme probabilities, making it more robust in cases where a single model has an unusually high or low probability.
- Balances between overfitting and underfitting: It often results in a model that strikes a balance between overfitting and underfitting, as it considers the entire distribution of model probabilities.

Cons:
- Complexity: Finding the MPM involves calculating the median over the model probabilities, which can be computationally intensive or challenging for complex models.
- May not align with predictive performance: While it reduces sensitivity to outliers, the MPM may not necessarily provide the best predictive performance.

Note that we can also extract the best model from the attribute in the fitted values as well.

### Best Predictive Model

In general, the HPM or MPM are not the best predictive models, which from a Bayesian decision theory perspective would be the model that is closest to BMA predictions under squared error loss.

Pros:
- Emphasizes predictive accuracy: BPM selects the model that gives the best predictive performance on average, making it a suitable choice when the ultimate goal is to make accurate predictions.
- Accounts for model uncertainty: It considers the predictive performance of all models, providing a more robust and uncertainty-aware approach.

Cons:
- Complexity: Implementing BPM can be computationally intensive, as it requires estimating predictive performance for each model in the set.
May not be as interpretable: The selected BPM might not be as easy to interpret as the HPM.

```{r}
BPM <- predict(model, estimator = "BPM")
variable.names(BPM)
```

Now we use ggpairs to see how these models compare:

```{r}
library(GGally)
GGally::ggpairs(data.frame(
  HPM = as.vector(HPM$fit), # this used predict so we need to extract fitted values
  MPM = as.vector(MPM$fit), # this used fitted
  BPM = as.vector(BPM$fit), # this used fitted
  BMA = as.vector(BMA$fit)
)) # this used predict
```

Using the `se.fit = TRUE` option with `predict` we can also calculate standard deviations for prediction or for the mean and use this as input for the `confint` function for the prediction object. Since there are many predictions, here we slice 15 samples and observe their confidence intervals:

```{r}
BPM <- predict(model, estimator = "BPM", se.fit = TRUE)
model.fit <- confint(BPM, parm = "mean")
model.pred <- confint(BPM, parm = "pred")
# take 15 samples
sample<-model.fit[1:15,]
class(sample) <- "confint.bas"
plot(sample)
```


```{r}
sample<-model.pred[1:15,]
class(sample) <- "confint.bas"
plot(sample)
```

## Prior Selection

BAS uses a model formula similar to lm to specify the full model with all of the potential predictors. Here we are using the shorthand . to indicate that all remaining variables in the data frame provided by the data argument. Prior selection is a critical step in Bayesian model averaging (BMA) as it determines the weights assigned to each model in the model space. The choice of prior distribution for the model parameters can significantly impact the BMA results. A well-informed prior can help avoid overfitting, reduce uncertainty, and improve model selection. However, selecting an appropriate prior can be challenging as it requires balancing between being informative enough to guide the model towards plausible solutions and being uninformative enough to avoid biasing the results. Prior elicitation techniques such as expert opinion, empirical data, and sensitivity analysis can be employed to guide the choice of priors. Overall, careful prior selection is crucial for obtaining reliable and accurate BMA results.
Different prior distributions on the regression coefficients may be specified using the prior argument, and include

- “BIC”
- “AIC
- "g-prior", Zellner's g prior where 'g' is specified using the argument 'alpha'
- "hyper-g", a mixture of g-priors where the prior on g/(1+g) is a Beta(1, alpha/2) as in Liang et al (2008). This uses the Cephes library for evaluation of the marginal likelihoods and may be numerically unstable for large n or R2 close to 1. Default choice of alpha is 3.
- "hyper-g-laplace", Same as above but using a Laplace approximation to integrate over the prior on g.
- "hyper-g-n", a mixture of g-priors that where u = g/n and u ~ Beta(1, alpha/2) to provide consistency when the null model is true.
- "JZS" Jeffreys-Zellner-Siow prior which uses the Jeffreys prior on sigma and the Zellner-Siow Cauchy prior on the coefficients. The optional parameter 'alpha' can be used to control the squared scale of the prior, where the default is alpha=1. Setting 'alpha' is equal to rscale^2 in the BayesFactor package of Morey. This uses QUADMATH for numerical integration of g.
- "ZS-null", a Laplace approximation to the 'JZS' prior for integration of g. alpha = 1 only. We recommend using 'JZS' for accuracy and compatibility with the BayesFactor package, although it is slower.
- "ZS-full" (to be deprecated)
- "EB-local", use the MLE of g from the marginal likelihood within each model
- "EB-global" uses an EM algorithm to find a common or global estimate of g, averaged over all models. When it is not possible to enumerate all models, the EM algorithm uses only the models sampled under EB-local.

Here are some recommendations for different priors under different situations, as well as pros and cons for different priors:

BIC:
- Use Case: BIC is often used for model selection in frequentist statistics. It penalizes model complexity and is appropriate when you want to balance model fit with model complexity.
- Pros: It is simple to compute and tends to select more parsimonious models.
- Cons: BIC may not perform well when the true model is not among the candidates, and it assumes that models are nested.
AIC:
- Use Case: AIC is also used for model selection and penalizes model complexity. It is useful when you want to balance model fit with model complexity.
- Pros: Like BIC, AIC is relatively simple to compute and can be used for a wide range of models.
- Cons: AIC can favor more complex models when the sample size is large, and it may not always be suitable for small sample sizes.
g-Prior (Zellner's g-prior):
- Use Case: The g-prior is commonly used when you want to specify a prior on the regression coefficients that balances between an uninformative prior (large 'g') and a more informative prior (small 'g').
- Pros: It allows you to control the strength of the prior information using the 'alpha' parameter. It provides a flexible way to incorporate prior beliefs.
- Cons: The choice of 'alpha' can impact results, and selecting an appropriate value may require prior knowledge.
Jeffreys-Zellner-Siow (JZS) Prior:
- Use Case: JZS prior combines the Jeffreys prior on the error variance and the Zellner-Siow Cauchy prior on regression coefficients. It's useful when you want a default informative prior.
- Pros: It provides a well-structured, informative prior that is robust in many situations.
- Cons: It may not be suitable if you have strong prior knowledge that conflicts with the default prior structure.
Empirical Bayes (EB) Priors:
- Use Case: EB priors are used when you want to estimate the hyperparameters (like 'g') from the data itself. EB-local estimates 'g' within each model, while EB-global estimates a common 'g' across all models.
- Pros: EB priors can adapt to the data and provide a data-driven prior.
- Cons: They may not perform well with very limited data, and the results can be sensitive to the choice of algorithm and prior distribution for hyperparameters.


Here we explore the model fitted under another prior, the g-prior to investigate how the prior selection affects the model output:

```{r}
g_model <-bas.lm(BMXBMI ~ .,
  data = nhanes_URX,
  prior = "g-prior",
  modelprior = uniform(), initprobs = "eplogp", method="MCMC",
  force.heredity = FALSE, pivot = TRUE
)
summary(g_model)
```

```{r}
coef.gmodel <- coef(g_model)
par(mfrow=c(2,2))
# plot(coef.model, mfrow=c(3,3), ask = F)
plot(coef.gmodel, ask = F)
```

Comparing this model output with the previous model output under the ZS-null prior, we notice that although the ZS-null prior model considers the `URXECP` predictor to be most important with an inclusion probability of 1.00, the g-prior model considers the `URXHIBP` predictor to be important with an inclusion probability of 0.9999. However, URXUCR, URXCNP, URXECP, URXHIBP, URXMBP and URXMEP were deemed important by both models. While the predictors have generally similar degrees of importance in both models, it is also worth noting that the posterior distributions of the URX variables have slightly different shapes under the two different priors.

## Why Bayesian Model Averaging? Advantages


### Model Uncertainty

-   It is important to take account of model uncertainty about statistical structure when making inferences. Oftentimes, there is remaining uncertainty not only about parameters, but also about the underlying true model. In this case, a Bayesian analysis allows one to take into account not only uncertainty about the parameters given a particular model, but also uncertainty across all models combined.

### Simultaneous Scenarios

-   Allows users to incorporate several competing models in the estimation process. In theory, BMA provides better average predictive performance than any single model that could be selected. BMA avoids the all-or-nothing mentality that is associated with classical hypothesis testing, in which a model is either accepted or rejected wholesale. In contrast, BMA retains all model uncertainty until the final inference stage, which may or may not feature a discrete decision.

### Model Misspecification

- BMA is relatively robust to model misspecification. If one does select a single model, then one had better be sure of being correct. With BMA, a range of rival models contribute to estimates and predictions, and chances are that one of the models in the set is at least approximately correct.

### References

https://www.jstor.org/stable/2676803?seq=3#metadata_info_tab_contents