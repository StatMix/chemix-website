---
title: "Tutorial 06 - Bayesian Model Averaging"
author: "Olivia Fan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    css: "./tutorials.css"
    toc: true
    toc_float: true
---

<!-- Setup -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning=FALSE,
                      message=FALSE)
```

## Topics

-   Basic ideas of Bayesian Model Averaging

-   Application of Bayesian Model Averaging using R package BAS

-   Estimation, Interpretation and Prediction

-   Advantages of Bayesian Model Averaging over all-or-none model selection

## Libraries

The R package `BAS` provides ways of carrying out BMA for linear regression, generalized linear models, and survival or event history analysis using Cox proportional hazards models. It contains functions for plotting the BMA posterior distributions of the model parameters, as well as an image plot function that provides a way of visualizing the BMA output. The functions `bas.lm` provide fast and automatic default ways of doing this for the model classes considered.

```{r}
library(BAS)
library(tidyverse)
library(broom.mixed)
```

To illustrate how BMA takes account of model uncertainty about the variables to be included in linear regression, we will be using the `nhanes` dataset where the variables are described in the file `nhanes-codebook.txt`. Load this data with the `load` function and specify the data file.

```{r}
load(file='nhanes1518.rda')
```

## Exploratory Data Analysis

We first explore the URX predictors (i.e. the ones related to phthalates concentrations), subsetting subset the dataset to include only the predictors in the weighted quantile sum module, and then filter out `NA` values:

```{r}
nhanes_URX<-nhanes1518%>%
  select(BMXBMI, URXUCR, URXCNP,URXCOP,URXECP,URXHIBP,URXMBP,URXMC1,URXMEP,URXMHBP,URXMHH)%>%
  na.omit()
```

We first start exploring the data by plotting a correlation matrix between the variables of interest. We can use the `corrplot` function within the `corrplot()` package, and adjust the style to fit our aesthetics of desire. We see that the predictors are highly correlated, especially between URXMHH and URXECP, and URXMHBP and URXMBP.

```{r}
library(corrplot)
corr_mat=cor(nhanes_URX,method="s")
corrplot(corr_mat, 
         addCoef.col = 1,    
         number.cex = 0.5) # Change font size of correlation coefficients
# other styles:
# default 
# corrplot(corr_mat) # circles
# squares, variables presented in an alphabet order
# corrplot(corr_mat, method = 'color', order = 'alphabet') # squares
# can choose different style for lower and upper triangle, can order the variable by clustering result
#corrplot.mixed(corr_mat, lower = 'shade', upper = 'pie', order = 'hclust')
```

```{r}
library(tidyverse)
df_long <- nhanes_URX %>% pivot_longer(everything())

# Plot histograms using ggplot
ggplot(df_long, aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ name, scales = "free")
```

We also plot the distributions of our variables of interest involved in the model, and notice that the variables have a long tail in the original scale, which hints at log transformation which we will perform later on.

## Bayesian Model Averaging

Motivated from chemical mixtures, one way to deal with highly correlated predictors is to select variable, or select model. Traditionally, analysis often proceeds by first selecting the best model according to some criterion and then learning about the parameters given that the selected model is the underlying truth. However, this approach has potential issues: (1) We cannot quantify model uncertainty through these all-or-none selection methods. (2) There are often many modeling choices that are secondary to the main questions of interest but can still have an important effect on conclusions. An alternative is to learn the parameters for *all* candidate models and then combine the estimates according to the posterior probabilities of associated model. *Bayesian Model Averaging* (BMA) carry this model combination idea - Specifically, this is done through a parameter estimate obtained by averaging the predictions of the different models under consideration, each weighted by its model probability. The math formula below illustrate below:

Given quantity of interest $\Delta$, such as an effect size, a future observation, or utility of a course of action, then its posterior distribution of given data D is 

$$pr(M_k|D) = \frac{pr(D|M_k)pr(M_k)}{\Sigma_{l=1}^k pr(D|M_l)pr(M_l)}$$

This is an average of the posterior distributions under each of the models considered, weighted by their posterior model probability. 

There are three packages available: `BAS`, `BMS`, and `BMA`. A thorough comparison are presented in this [paper](https://www.researchgate.net/profile/Christopher-Parmeter/publication/268216833_Bayesian_model_averaging_in_R/links/54ac4c040cf2479c2ee7b14e/Bayesian-model-averaging-in-R.pdf).  In this module, we will investigate this method known as *Bayesian Model Averaging* using `BAS` package. 

## Bayesian Model Averaging for Linear Regression Models

> Explain the math formula and say we consider Mk to be linear regression then provide the linear model formula here from this [website](https://cran.r-project.org/web/packages/BMS/vignettes/bmsmanual.pdf)  

We fit a BMA model on the dataset using BMI as the response variable, and all of the 10 URX variables as predictor variables, after log transforming both variables: 

As seen in the EDA, we perform log transformation due to the long tail and extreme values in the original scale.


```{r}
nhanes_URX<-log(nhanes_URX)
model <-bas.lm(BMXBMI ~ .,
  data = nhanes_URX,
  prior = "ZS-null",
  modelprior = uniform(), initprobs = "eplogp", method="MCMC",
  force.heredity = FALSE, pivot = TRUE
)
summary(model)
```

> When introduce the functions, also explain what the arguments do

> Explain the output of `summary()`. What does each column represent? How does r2, BIC, post prob help us to specify the important variables or the model? Also comment on what valuable information we obtain. For example, what are the important variables? What questions it help us answer?

> We need to provide example on how to dig insights using the model instead of purely presenting the R coding. Read the Section 7.2 Example 2 of Bayesian Model Averaging: A Tutorial carefully. Please also read the application part of this paper carefully: [Bayesian Model Averaging: Theoretical Developments and Practical Applications](https://www.cambridge.org/core/journals/political-analysis/article/bayesian-model-averaging-theoretical-developments-and-practical-applications/3179D92A3C9353DE7E4674987C33FD28)

In R, the `image` function may be used to create an image of the model space that looks like a crossword puzzle:

```{r}
image(model, rotate=F)
```

Here, the predictors, including the intercept, are on the y-axis, while the x-axis corresponds to each different model. The red color indicates when the variable estimate is positive, the blue color indicates when the variable estimate is negative, and the beige color is the color to use when the variable is not included in the model. Here the imageplot indicates that most of the URX variables are significant in predicting the BMI level, except for `URXMC1` which is not included in all of the six models produced by BMA.

If we view the image by rows, we can see whether one variable is included in a particular model. For each variable, there are only 6 models in which it will appear. For example, we see that `URXUCR` and `URXMEP` appear in all the models. `URXMHH` appears in the top 2 models with larger posterior probabilities, but not the last 4 models.

We can also plot the posterior distributions of these coefficients to take a closer look at the distributions:

```{r}
coef.model <- coef(model)
plot(coef.model, mfrow=c(3,3), ask = F)
```

This plot agrees with the summary table we obtained above, which shows that the posterior probability distributions of `URXMC1` and `URXCOP` have a very large point mass at 0, while the distribution of `URXUCR` and `URXMEP` have relatively small mass at 0. There is a slighly little tip at 0 for the variable `URXUCR`, indicating that the posterior inclusion probability of `URXECP` is not exactly 1. However, since the probability mass for `URXUCR` to be 0 is so small, that we are almost certain that `URXECP` should be included under Bayesian model averaging.

## Bayesian Model Diagnostics

```{r}
plot(model, ask = F)
```

```{r}
diagnostics(model, type = "pip", pch = 16) 
```

```{r}
plot(confint(coef.model),estimator = "HPM") # also look into this
```

## Bayesian Model Predictions

With the `predict` function, we can understand uncertainty besides classification, taking a look at the predictive probabilities for the two class classification problem:


## Bayesian Model Predictions with Model Selection

> add here

## Prior Selection


## Why Bayesian Model Averaging? Advantages

### Model Uncertainty

-   It is important to take account of model uncertainty about statistical structure when making inferences. Oftentimes, there is remaining uncertainty not only about parameters, but also about the underlying true model. In this case, a Bayesian analysis allows one to take into account not only uncertainty about the parameters given a particular model, but also uncertainty across all models combined.

### Simultaneous Scenarios

-   Allows users to incorporate several competing models in the estimation process. In theory, BMA provides better average predictive performance than any single model that could be selected. BMA avoids the all-or-nothing mentality that is associated with classical hypothesis testing, in which a model is either accepted or rejected wholesale. In contrast, BMA retains all model uncertainty until the final inference stage, which may or may not feature a discrete decision.

### Model Misspecification

-   BMA is relatively robust to model misspecification. If one does select a single model, then one had better be sure of being correct. With BMA, a range of rival models contribute to estimates and predictions, and chances are that one of the models in the set is at least approximately correct.

### References

https://www.jstor.org/stable/2676803?seq=3#metadata_info_tab_contents