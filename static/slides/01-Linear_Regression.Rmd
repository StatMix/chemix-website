---
title: "Tutorial 02 - Linear Regression"
author: "Olivia Fan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    css: "./tutorials.css"
    toc: true
    toc_float: true
---

<!-- Setup -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning=FALSE,
                      message=FALSE)
```

## Topics

-   Use simple linear regression to describe the relationship between a quantitative predictor and quantitative response variable.

-   Estimate the slope and intercept of the regression line using the least squares method.

-   Interpret the slope and intercept of the regression line.

## 1. Motivation & Library

The `library()` function is used to access functionality that is provided by R packages, but is not included in base R. `install.packages()` can be used to install new packages. Run this command from the console.

```{r}
# install.packages("tidyverse")
```

First, load the package `tidyverse` that will be used throughout the tutorial for data visualizations.

```{r}
library(tidyverse)
```

```{r}
load(file='nhanes1518.rda')
```

*The functions `head()` and `names()` can be used to explore the data. `head()` can output the first several rows of the data, while \``names()` can provide all the names of variables.*

```{r}
head(nhanes1518)
names(nhanes1518)
```

In this chapter, we introduce linear regression which aims to model the relationship between a response variable and one or more predictor variables by fitting a linear equation to observed data. The goal of linear regression is to find the best-fitting line (or hyperplane) that minimizes the sum of the squared differences between the observed responses and the values predicted by the equation. The resulting equation can then be used to make predictions about the response for new inputs. In essence, linear regression aims to answer the question of how changes in the independent variables relate to changes in the dependent variable.

We will illustrate investigating the relationship between age and BMI as an example of linear regression.

## 2. Exploratory Data Analysis

We are interested in the relation between age and BMI, and are especially interested in adults. Before applying data analysis, we want to explore the distribution of these variables:

```{r}
nhanes1518 <- nhanes1518%>%filter(RIDAGEYR>=18)
```

This tutorial will be using the `nhanes` dataset where the variables are described in the file `nhanes-codebook.txt`. Load this data with the `load` function and specify the rda data file.

```{r}
# Basic histogram
ggplot(nhanes1518, aes(x=RIDAGEYR)) + 
  geom_histogram(binwidth=2, color="black", fill="white")+ # Change the width of bins as well as the color
  labs(x = "Age", title = "Distribution of Age") # legend of the plot
```

We are interested in the relation between age and BMI. Before applying data analysis, we want to check the distributions of these variables.

We see that the distribution of waist circumference is asymmetric, with peaks at age of around 80 and 0 respectively.

We can also add the mean line, as well as overlay with transparent density plot. The value of alpha controls the level of transparency:

```{r}
# Histogram with density plot
ggplot(nhanes1518, aes(x=RIDAGEYR)) + 
 geom_histogram(binwidth=2, aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ # add a layer of density
  labs(x = "Age", title = "Density of Age") +
  geom_vline(aes(xintercept=mean(RIDAGEYR)),
            color="blue", linetype="dashed", size=1) # Add mean line
```

Similarly, we can explore the distribution of BMI, which we found to be right-skewed:

```{r}
# Basic histogram
ggplot(nhanes1518, aes(x=BMXBMI)) + 
  geom_histogram(binwidth=2, color="black", fill="white")+ # Change the width of bins as well as the color
  labs(x = "BMI", title = "Distribution of BMI") # legend of the plot
```

We can also explore the scatterplot of the relationship between age and BMI:

```{r}
# Here the dollar sign specifies that we retrieve the predictor columns `BMXBMI` and `BMXWAIST` from the dataset `nhanes1518`
# We can experiment with different options for `abline()` by changing the line width and color in `abline()`.
ggplot(nhanes1518, aes(x = RIDAGEYR, y = BMXBMI)) + 
    geom_point(size=1, color="dark blue", pch = 20) + # size adjust point size; pch (plotting character) adjusts shape of the points 
  labs(title="Age vs. BMI for the nhanes data", x="Body Mass Index (kg/m**2)",y="Waist Circumference (cm)")
```

## 3. Model Assumptions

The assumptions of linear regression model are as follows:

-   *Linearity*: The relationship between the independent and dependent variables is linear.
-   *Independence*: The observations are independent of each other, meaning that the value of the dependent variable for one observation is not influenced by the values of the independent variables for other observations.
-   *Homoscedasticity*: The variance of the errors is constant for all values of the independent variables.
-   *Normality*: The errors are normally distributed.
-   *No multicollinearity*: The independent variables are not highly correlated with each other.
-   *No Autocorrelation*: The errors are not correlated with each other. Violations of these assumptions can affect the accuracy and interpretability of the regression results. It is important to check for these assumptions before applying linear regression and consider alternative models if necessary.

We will verify that our data meets all the assumptions above in Section 5. Model Diagnostics.

## 4. Linear Regression

### Simple Linear Regression

Simple linear regressions take the form

$$Y_i = \beta_0 +\beta_1 X_i +\epsilon_i$$ Where $Y_i$ is the dependent variable, $X_i$ is the independent variable and $\epsilon_i$ is the random error term.

-   $\beta_1$: True slope of the relationship between X and Y
-   $\beta_0$: True intercept of the relationship between X and Y
-   $\epsilon$: Error (residual)

We'll start with a fitting a simple linear model using the `lm()` function. In the `lm()` function, the first variable is the response variable and the variables to the right of the `~` symbol are the predictor variable(s). Here we use age as the response, and BMI as the predictor variables.

```{r}
lm.fit <- lm(BMXBMI ~ RIDAGEYR, data = nhanes1518)
```

There are several ways that we can examine the model results. The `summary()` function gives a more extensive overview of the model fit:

```{r}
summary(lm.fit)
```


### Model Interpretation

- $\beta_0$: Not interpretable in our case. For a person with 0 age (which falls outside of the domain of our model since we only consider adults), his or her expected age is 0.161 (which we refer to as extrapolation since the value of the predictor variable falls outside of our domain of concern). $\beta_1$: For every unit increase in the BMI of a person, his or her age is expected to increase by 1.432 on average.

-   p value: The p value tells us how likely the data we have observed is to have occurred under the null hypothesis (more material on Null hypothesis on subsequent tutorials), i.e. that there is no correlation between the predictor variable age and the response BMI. From the model above, we have a p value of less than 2e-16, which tells us that the predictor variable age is statistically significant.


The coefficients of the linear regression model can be extracted using the `coef()` function and the confidence interval(s) with the `confint()` function.

```{r}
coef(lm.fit)
confint(lm.fit)
```

### Model Prediction

We can use the `predict()` function to obtain prediction intervals or confidence intervals for a given value of the predictor variable, `BMXWAIST`. Note that when using the predict function, the column names and format of the new points at which to predict needs to be the same as the original data frame used to fit the `lm()` model. If you encounter errors using the `predict()` function, this is a good first thing to check.

```{r}
predict(lm.fit, data.frame(RIDAGEYR = (c(18, 30, 60))), interval = "confidence")
predict(lm.fit, data.frame(RIDAGEYR = (c(18, 30, 60))), interval = "prediction")
```

### Prediction Interval vs Confidence Interval

Prediction and confidence interval are both statistical concepts that are used to estimate or quantify uncertainty in a particular outcome or parameter. However, they have different meanings and interpretations. 

- Confidence interval: a range of values that is likely to contain the true value of a parameter with a certain degree of confidence. For example, if you are estimating the mean age of a population based on a sample, a 95% confidence interval would provide a range of values within which the true population mean age is expected to lie with 95% confidence. The width of the confidence interval reflects the uncertainty in the estimation, with wider intervals indicating more uncertainty.

- Prediction: An estimate of a specific value or outcome based on the statistical model. For example, in our case when we use a linear regression model to predict the age of a person based on their BMI, a prediction would be the estimated age for a person of a specific BMI Predictions are usually accompanied by a measure of how accurate the estimate is, such as the mean squared error or the root mean squared error. Unlike a confidence interval which provides a range of values for the parameter, a prediction interval provides a range of values for the actual observation or outcome.

We can plot the variables `BMXBMI` and `BMXWAIST` using the `ggplot` function and overlay the regression line found using `lm` with the `geom_smooth()` function.

```{r}
# Here the dollar sign specifies that we retrieve the predictor columns `BMXBMI` and `BMXWAIST` from the dataset `nhanes1518`
# We can experiment with different options for `abline()` by changing the line width and color in `abline()`.
ggplot(nhanes1518, aes(x = RIDAGEYR, y = BMXBMI)) + 
    geom_smooth(method = "lm", formula = y ~ x, colour = "red") + 
    geom_point(size=1, color="dark blue", pch = 20) + # size adjust point size; pch (plotting character) adjusts shape of the points 
  labs(title="Age vs. BMI for the nhanes data", x="Body Mass Index (kg/m**2)",y="Waist Circumference (cm)")
```

## 5. Model Diagonostics & Interpretation

The `par()` function can be used to create a grid of multiple subplots. While the `plot()` function provides a convenient way to create the diagnostic plots, they can also be manually created. We use the `plot()` function below to demonstrate its usage:

```{r}
par(mfrow = c(2, 2))
plot(lm.fit)
```

The diagnostic plots show residuals in four different ways:

-   Residuals vs Fitted. Used to check the linear relationship assumptions. A horizontal line, without distinct patterns is an indication for a linear relationship, what is good. The model we fitted shows roughly a linear relationship, with no distinct patterns (such as a fan or funnel shape) in the residuals vs. fitted plot.

-   Normal Q-Q. Used to examine whether the residuals are normally distributed. It's good if residuals points follow the straight dashed line. The Q-Q plot generally follows the straight dashed line, with some deviations at the end towards high values of theoretical quantiles.

-   Scale-Location (or Spread-Location). Used to check the homogeneity of variance of the residuals (homoscedasticity). Horizontal line with equally spread points is a good indication of homoscedasticity.

-   Residuals vs Leverage. Used to identify influential cases, that is extreme values that might influence the regression results when included or excluded from the analysis. Based on the residuals vs. leverage plot, there are no influential points according to Cook's distance. However, there might be some points with high standard residuals values which could be marked as outliers.

Some other metrics from the model:

-   $R^2$: From the model above, we have an adjusted R-squared value of 0.2302, which indicates that 23.02% of the variability in the response variable BMI can be explained by the change in the predictor variable age.
-   p value: The p value tells us how likely the data we have observed is to have occurred under the null hypothesis (more material on Null hypothesis on subsequent tutorials), i.e. that there is no correlation between the predictor variable age and the response BMI. From the model above, we have a p value of 2.2e-16, which tells us that the predictor variable age is statistically significant.

We can use the `residuals()` and `rstudent()` functions to extract the residuals and studentized residuals, respectively, from the linear model and plot them along with the predicted values.

```{r}
plot(predict(lm.fit), residuals(lm.fit))
```


## 6. Multiple Linear Regression

The `lm()` function can also fit multiple regression models. In this section, we will use `RIDAGEYR`, `BMXWAIST`, `BMXWT`, and `BMXHT` as predictors of the response variable `BMXBMI`.


Multiple linear regression allows to evaluate the relationship between two variables, while controlling for the effect (i.e., removing the effect) of other variables.

```{r}
lm.fit <- lm(BMXBMI ~ RIDAGEYR+ BMXWAIST + BMXWT + BMXHT, data = nhanes1518)
summary(lm.fit)
```

### Model Interpretation:

-   Intercept: The intercept does not have interpretability since it is unrealistic to have age 0, body waist circumference of 0, height and weight of 0.
-   `BMXWT`: The coeffcient for the predictor `BMXWT` is 0.2420969, which means that for every unit increase in the participant's waist circumference, the BMI is expected to increase by 0.2420969 on average, holding all else constant (holding all other predictor variables, `BMXWAIST`, `BMXWT`, and `BMXHT` constant)


```{r}
# we can use select to filter the variables of interest
nhanes_core<-nhanes1518%>%select(BMXBMI, RIDAGEYR, BMXWAIST,BMXWT,BMXHT)
# In the lm() formula, a dot . can be used to include all variables in the NHANES data as predictors.
lm.fit1 <- lm(nhanes1518$BMXBMI ~ ., data = nhanes_core)
summary(lm.fit1)
```

```{r}
# If we want to exclude specific variables from the list of predictors, we can use the `-` notation. In the following example, all predictor variables but `age` are included in the model.
# Including `-1` excludes the intercept from the model.

lm.fit1 <- lm(nhanes1518$BMXBMI ~ .- 1, data = nhanes_core)
summary(lm.fit1)
```


<!-- ### Multilinear Regression -->

<!-- Multiple linear regression allows to evaluate the relationship between two variables, while controlling for the effect (i.e., removing the effect) of other variables. -->

<!-- ```{r} -->
<!-- multilm.fit <- lm(RIDAGEYR ~ BMXBMI+WTINT4YR, data = nhanes1518) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- summary(multilm.fit) -->
<!-- ``` -->

### Multicollinearity Diagnostics

Apart from diagnostics for simple linear regression, we also need to perform multicollinearity checks for multiple linear regression:

```{r}
library(car)
# Calculate VIFs
vifs <- vif(lm.fit)

# Print VIFs
print(vifs)
```

Variance Inflation Factor (VIF): The VIF is a measure of the increase in the variance of the estimated coefficients due to multicollinearity. A VIF value greater than 5 indicates that there is strong multicollinearity in the model. Here the multicollinearity issue is not significant.


## 7. Categorical Variable

```{r}
nhanes_na_removed<-cbind(nhanes1518[1:5],nhanes1518$BMXBMI, nhanes1518$INDHHIN2)
nhanes_income <- rename(nhanes_na_removed,income = "nhanes1518$INDHHIN2")%>% na.omit()
nhanes_income <- rename(nhanes_income,BMI = "nhanes1518$BMXBMI")
# turn quantitative variable into categorical variable
nhanes_income$income<-as.character(nhanes_income$income)
```

Now we explore the effect of income on the response BMI. Income is stored as `1`, `2`,.., `15`, `77`, `99`, a categorical predictor variable, in the dataset. The encoding of income categories can be found in this website:

<https://wwwn.cdc.gov/nchs/nhanes/2011-2012/demo_g.htm#INDHHIN2>

We want to first drop categories with values 77 (Refused) and 99 (Don't Know) first:

```{r}
nhanes_income <- nhanes1518%>%select(BMXBMI, RIDAGEYR,INDHHIN2)%>%rename(income=INDHHIN2)%>%
  filter(!income %in%c("77","99"))
nhanes_income$income<-as.factor(nhanes_income$income)
```

Then we fit the linear regression on categorical variable `income`:

```{r}
summary(lm(BMXBMI ~ income, data = nhanes_income))
```

*Baseline*: income category 1 corresponding to a household income of 0 to 4,999 dollars.

*Model Interpretation*:

-   Intercept: The intercept 25.6489 means that for people in the baseline income category (income category 1 corresponding to a household income of 0 to 4,999 dollars), the BMI is expected to be 25.6489 on average.

-   `income6`: The coeffcient for the predictor `income6` is 1.1473, which means that for participants with household income category 6 (25,000 to 34,999 dollars per ear), the BMI is expected to be 1.1473 higher than that of participants with household income in category 1 (0 to 4,999 dollars), on average.

## 8. Interaction Terms

There are two ways to include interaction terms in the model, `:` and `*`. The `:` symbol only includes the interaction term between the two variables, while the `*` symbol includes the variables themselves, as well as the interaction terms. This means that `BMXWT*BMXWAIST` is equivalent to `BMXWT + BMXWAIST + BMXWT:BMXWAIST`.

```{r}
summary(lm(BMXBMI ~ income+RIDAGEYR+income*RIDAGEYR, data = nhanes_income))
```

```{r}
# Plot the interaction effects
df<-nhanes_income%>%
  filter(income %in%c("1", "10"))
ggplot(df, aes(x = RIDAGEYR, y = BMXBMI, color = income)) + 
  geom_point(alpha=0.5) + 
  stat_smooth(method = "lm", se = FALSE, fullrange = TRUE)+
  labs(x = "Waist Circumference", y = "BMI") + 
  ggtitle("Interaction effects between income and age on BMI")
```

The plot shows the relationship between BMI and waist circumference for two levels of income, level 1 (annual income of 0-4999 USD) and level 10 (annual income of 65,000 to 74,999 USD), where each level is represented by a different color. The plot provides a visual representation of how the effect of waist circumference on BMI differs for each level of income: For income category 1 with annual income of 0-4999 USD, we observe a steeper slope, which suggests that for each unit increase in BMI, we expect a higher rise in waist circumference if the individual is in income category 1, compared to if the individual is in income category 10. Conversely, for income category 10 with annual income of 65,000 to 74,999 USD, we observe a flatter slope, which suggests that for each unit increase in BMI, we expect a lower rise in waist circumference if the individual is in income category 10, compared to if the individual is in income category 1.

```{r}
# select our variables of interest
df<-nhanes1518%>%
  select(SEQN, WTINT2YR, WTMEC2YR, RIDSTATR, RIAGENDR, BMXBMI)
# data clearning to ignore NA values
nhanes_na_removed<-df%>%na.omit()
# A simple way to include all interaction terms is the syntax `.^2`
summary(lm(BMXBMI ~ .^2, data = nhanes_na_removed))
```

## Related Topics

| **Topics**         |       **Concepts**        |
|--------------------|:-------------------------:|
| ANOVA              |      Null hypothesis      |
| Quantile factoring | use qualitative as levels |
| Cross-validation   |       k-fold, LOOCV       |
| Model section      |       AIC, BIC, Cp        |
