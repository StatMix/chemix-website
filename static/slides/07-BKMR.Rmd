---
title: "Work in progress: Bayesian Kernel Machine Regression"
author: "Bradley Bowen"
date: "Last Updated `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    css: "./tutorials.css"
    toc: true
    toc_float: true
---

## Introduction

Often the case with health data, mixtures may have non-linear relationships with the outcome variable, meaning that standard additive models may not be best suited. Working with highly correlated exposures and complex interaction effects with previous models could potentially lead to inconsistent estimates. In this tutorial, we will explore Bayesian Kernel Machine Regression (BKMR) which allows for increased flexibility for complex relationships between the predictor and outcome variables.

Kernel Machine Regression provides a more flexible approach by using a kernel. Kernels in regression are beneficial as they provide an estimation of the relationship between the predictors and outcome without knowing the form (linear, logistic, exponential, etc). This means that the kernel is a non-parametric technique. In order to determine the best fit between the predictor variables and outcome of interest, the kernel weights data points that are close in distance to outcome higher, and points that have a greater distance lower. This method allows the regression model to better fit the data and account for complex relationships between variables.

Additionally, the Bayesian Kernel Machine Regression integrates Kernel Machine Regression in a Bayesian framework, placing priors on model parameters, providing uncertainty quantification through the posterior, and allowing for flexibility such as hierarchical variable selection.

## Preparation

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(ggcorrplot)
library(corrplot)
```

To illustrate how BKMR empowers modeling of highly correlated predictors, we will use the `nhanes` dataset where the variables are described in the file `nhanes-codebook.txt`. Load this data with the `load` function and specify the data file.

```{r}
load(file='nhanes1518.rda')
```

We first explore the URX predictors (i.e. the ones related to phthalates concentrations), subsetting subset the dataset to include only the predictors in the weighted quantile sum module, and then filter out `NA` values:

```{r}
nhanes<-nhanes1518 |>
  select(BMXBMI,URXMHBP, URXMOH, URXMHP, URXMHH, URXMCOH, URXMHNC, URXMHH, URXECP, URXHIBP, URXMIB, RIDAGEYR)|>
  na.omit()|>
  mutate(BMXBMI = log(BMXBMI))

```

We first start exploring the data by plotting a correlation matrix between the variables of interest. We can use the `corrplot` function within the `corrplot()` package, and adjust the style to fit our aesthetics of desire. We see that the predictors are highly correlated, especially between URXMHH and URXECP, and URXMHBP and URXMBP:

```{r}
corr_mat=cor(nhanes|>select(-RIDAGEYR),method="s")
corrplot(corr_mat, # colorful number
         addCoef.col = 1,    # Change font size of correlation coefficients
         number.cex = 0.5) 
```

We also plot the distributions of our variables of interest involved in the model, and notice that the variables have a long tail in the original scale, which hints at log transformation which we will perform later on.

```{r}
df_long <- nhanes|>select(-RIDAGEYR) |> pivot_longer(everything())

# Plot histograms using ggplot
ggplot(df_long, aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ name, scales = "free")

```

## Bayesian Kernel Machine Regression

Bayesian Kernel Machine Regression (BKMR) model the relationship between outcome and variables using two parts: a nonparametric function $h(\cdot)$ and linear function.

$$Y_i = h(\bf{Z}_i)+X_i\beta + \epsilon_i$$

where $\bf{Z}_i = (Z_{1i},\dots,Z_{ki})$ are chemical mixtures of observation $i$, $h(\cdot)$ is named predictor-response function, a nonparametric function capturing the nonlinear relationship between chemical mixtures and outcome, and $X_i$ represent other variables which may have linear relationship with outcome and $\beta$ corresponds to coefficient. $\epsilon_i \overset{i.i.d.}{\sim} N(0,\sigma^2)$ is the error term.

The nonparametric function $h(\cdot)$ is highly flexible, and usually represented using kernel function:

$$h(\bf{Z}) = \sum_{i=1}^n K(Z_i,Z) \alpha_i$$

where $K(Z,Z')$ is a function measures the similarity of $Z$ and $Z'$, $\alpha_i$'s are coefficients. The outcome corresponding to $X$ are weighted by similarity of $Z$ to the data points $\{Z_i,i = 1,\dots,n\}$, data points that are more similar/ closer to $Z$ will plays more important role in outcome estimation. A common choice is a Gaussian kernel

$$K(Z,Z') = exp\{-\sum_{k=1}^K(Z_k-Z_k')^2/\rho\}$$.

The BKMR can also allow for variable selection by slightly modifying the kernel:

$$K(Z,Z';r) = exp\{-\sum_{k=1}^Kr_k(Z_k-Z_k')^2\}$$

where $r_k$ are non-negative values, with a "slab-and-spike" prior. If $r_k=0$, the variable $Z_k$ are excluded from the model.

### Model Fitting

Similar to previous tutorials, we are interested in the research question: predicting log BMI using phthalate measurements from urine samples for individuals above 18 years of age. We assume the age has linear relationship to logBMI. For illustration, we randomly sample 10% from the dataset.

```{r}
set.seed(1)
nhanes = nhanes |> sample_frac(size = 0.1)
y<-nhanes$BMXBMI # bmi
Z <- nhanes |> select(-BMXBMI, -RIDAGEYR) # chemical mixtures
Z <- log(Z)
# group 1: URXMBP, URXMIB, URXMC1, URXMEP, URXMZP
# group 2: URXECP, URXMHH, URXMOH, URXMHP
X <- nhanes |> select(RIDAGEYR)
Z <- as.matrix(Z)
X <- as.matrix(X)
```

To fit the BKMR model, we use the `kmbayes` function in R package `bkmr`.

```{r}
#install.packages("bkmr")
library(bkmr)
```

In the model:

-   `y` is a vector of the response

-   `Z` is the matrix containing chemical mixtures (variables with nonlinear relationship to outcome)

-   `X` is the matrix containing other covariates with linear relationship to outcome

-   `iter` number of iterations of the MCMC sampler

-   `varsel`, whether to conduct variable selection on the predictors $Z$.

-   `verbose`, printing the details during modeling fitting

In this example, we set `varsel = TRUE` to conduct variable selection while fitting the model.

```{r cache=TRUE}
set.seed(111)
fitkm <- kmbayes(y = y, Z = Z, X = X, iter = 5000, varsel = TRUE, verbose = FALSE)
```

### Model Interpretation and Inference

#### Estimating Parameters

`ExtractEsts()` provides the summary statistics (mean, standard deviation, quantiles) of posterior samples of parameters $\beta$, $r_k$, and $\sigma^2$. We can use `q_2.5` and `q_97.5` (2.5% and 97.5% quantiles) to construct 95% credible interval.

```{r}
ExtractEsts(fitkm)
```

We can say for the selected sample, holding other variables constant, for each additional year of age, 95% of the posterior distribution for the percentage change in BMI is between `r round((exp(ExtractEsts(fitkm)$beta[3])-1)*100,3)`% and `r round((exp(ExtractEsts(fitkm)$beta[7])-1)*100,3)`%.

#### Estimating posterior inclusion probability

If we conduct variable selection (`varsel = TRUE`) while fitting the model, we can use `ExtractPIPs()` to extract the posterior inclusion probabilities(PIP). The PIPs represent the probability that a specific predictor is included in the model, providing a quantitative measure of the predictor's importance.

For the selected sample, only URXMHP has PIP greater than 0.5, suggesting URXMHP has strongest potential effect to BMI. URXMHH, URXHIBP has PIP close to 0, suggesting these two chemical mixtures have little potential effect to BMI when other chemical mixtures are presented.

```{r}
ExtractPIPs(fitkm)
```

Alternatively, we can also use `summary()` to summarize BKMR model fits. Here we set `show_MH=FALSE` to avoid printing acceptance rates from Metropolis-Hastings algorithm. The acceptance rate can be used to evaluate convergence and mixing of MCMC sampler. We will discuss it in the [MCMC Diagnostics]{#eva} section.

```{r}
summary(fitkm,show_MH = FALSE)
```

Notice, we cannot directly interpret parameters $r$. Instead, we can estimate $h(\cdot)$ to interpret the relationship between outcome and chemical mixtures

#### Estimating h

The function $h(\bf{Z})=h((Z_1,\dots,Z_k))$ is called predictor-response function, capturing how the predictors (chemical mixtures) influence the response (outcome variable (log(BMI))). This function allows us to account for nonlinear relationships as well as potential interactions.

Notice $h(\cdot)$ is a high-dimensional surface which cannot be visualized, and it takes a nonparametric form which is hard to interpret. Here we introduce three different ways to explore $h(\dot)$:

-   Estimate h at a given value of predictors: $h(Z=z)$

-   Uni/Bi-variable predictor-response function: holding other predictors constant, exploring relationship of one or two predictors (chemical mixtures) with the outcome, which is $h(Z_k = z_k|Z_{(-k)}=Z_{(-k),med})$ or $h(Z_k = z_k, Z_m = z_m|Z_{(-k,-m)}=Z_{(-k,-m),med})$.

-   Overall risk summaries: calculating the overall effect of predictors by comparing $h(Z=z)/h(Z=Z_{med})$.

##### Estimate h at a given value of predictors

Notice the BKMR can be rewritten as follows:

$$y_i \sim N(h_i+x_i^T\beta,\sigma^2)$$ $$\bf{h}=(h_1,\dots,h_n)^T \sim N(0,\tau K)$$

where $K$ is a $n$ by $n$ kernel matrix, with $(i,j)$-element $K(z_i,z_j)=exp{-\sum_{k=1}^Kr_k(z_{ik}-z_{jk})^2}$.

Derived from this model, the posterior distribution of $h(\cdot)$ is normally distributed with mean $\mu_h(\theta)$ and variance $V_h(\theta)$, where $\theta=(\beta,r,\lambda)$ are parameters.

The `BKMR` package can estimate $h(\cdot)$ using three methods:

1.  `ComputePostmeanHnew(method = "approx")`. $h \sim N(\hat{\mu}_h,\hat{V}_h)$, where $\hat{\mu}_h$ and $\hat{V}_h$ are approximated by plugging in posterior mean of $\theta$: $\mu_h(\hat{\theta})$, $V_h(\hat{\theta})$.

2.  `ComputePostmeanHnew(method = "exact")`. $h \sim N(\hat{\mu}_h,\hat{V}_h)$, where $\hat{\mu}_h$ and $\hat{V}_h$ are $E(\mu_h(\theta))$ and $E(V_h(\theta))+Var(\mu_h(\theta))$ estimated by posterior samples.

3.  `SamplePred()`. $h \sim N(\mu_h,V_h)$. Instead of estimating posterior mean and variance, we directly sample $h$ given posterior samples of $\theta$.

The first approach is fast but only an approximation. The second and third approaches are exact methods that can provide unbiased estimates of posterior summaries, and the third approach can provide full posterior distribution. But these two exact methods take much longer time especially the third one. We recommend to use the third one for small dataset, the second one for moderate sized datasets and the first one for large dataset.

The three methods are demonstrated below using the median values of the Phalates and Phytoestrogens from the NHANES dataset.

```{r}
# calculate the median of the Z's
med <- apply(Z, 2, median)
Znew <- matrix(med, nrow = 1)
# approach 1: 
h_est1 <- ComputePostmeanHnew(fitkm, Znew = Znew, method = "approx")
# approach 2:
h_est2 <- ComputePostmeanHnew(fitkm, Znew = Znew, method = "exact")
# approach 3:
set.seed(1)
samps3 <- SamplePred(fitkm, Znew = Znew, Xnew = cbind(0))

compare <- data.frame(
  method = c(1:3),
  post_mean = c(h_est1$postmean, h_est2$postmean, mean(samps3)),
  post_sd = c(sqrt(h_est1$postvar), sqrt(h_est2$postvar), sd(samps3))
)

print(compare)
```

Both the second and third methods produced similar posterior means. However, the first "approximation" method differed slightly from the remaining approaches, and underestimates the standard deviation.

##### Uni/Bi-variable predictor-response function

`PredictorResponseUnivar()` captures relationship between $h$ and single predictor, while fixing other predictors at their median. We observe that for predictors with small PIP(XXXX), the posterior means (blue line) of $h(\cdot)$ are horizontal line at 0, indicating that they do not have a large impact on the response. The URXMOH, XXXXX. The URXMHP, XXXX.

TODO: mention the method = "" and the default method is.

```{r}
pred.resp.univar <- PredictorResponseUnivar(fit = fitkm)
ggplot(pred.resp.univar, aes(z, est, ymin = est - 1.96*se, ymax = est + 1.96*se)) + 
    geom_smooth(stat = "identity") + 
    facet_wrap(~ variable) +
  ylab("h(z)")
```

To investigate relationship between $h$ and two predictors, we can plot the relationship between outcome and the first predictor given the second predictor at varying quantile levels, holding other predictors at a fixed quantile level (we use median by default). We need to apply `PredictorResponseBivar()` to obtain relationship between outcome and two predictors, then apply `PredictorResponseBivarLevels()` to obtain relationship between outcome and first predictor conditional on the second predictor.

Visualizing relationship between $h$ and two predictors can help us check the interactions between two predictors. We observe that only for row `URXMHP` and `URXMOH`, there are clear difference among the lines in different colors. This indicates XXXX. We can check whether there exists some interactions between predictors from the plot. If the lines with different color has XXXX .

```{r}
pred.resp.bivar <- PredictorResponseBivar(fit = fitkm, min.plot.dist = 1)
pred.resp.bivar.levels <- PredictorResponseBivarLevels(
  pred.resp.df = pred.resp.bivar, Z = Z, qs = c(0.1, 0.5, 0.9))
ggplot(pred.resp.bivar.levels, aes(z1, est)) + 
    geom_smooth(aes(col = quantile), stat = "identity") + 
    facet_grid(variable2 ~ variable1) +
    ggtitle("h(expos1 | quantiles of expos2)") +
  xlab("expos1") +
  theme_bw()
```

##### Overall risk summaries

Summary statistics of the exposure-response function h(z) can be also examined and visualized. For example, we can examine how each phthalate contributes to the outcome BMI by comparing the effect at different percentiles while fixing the other variables to specified percentiles. In this example, we examine how the chemical mixtures differ at the 25th, 50th, and 75th percentile. The built-in `SingVarRiskSummaries` function can be used along with `qs.diff` which indicates which quantiles are being compared. Finally, the `q.fixed` argument allows the user to specify what percentiles to fix the other variables.

TODO: pick one to interpret XXXX

```{r}
risks.overall <- OverallRiskSummaries(fit = fitkm, y = y, Z = Z, X = X, 
                                      qs = seq(0.25, 0.75, by = 0.05), 
                                      q.fixed = 0.5, method = "exact")
risks.overall
ggplot(risks.overall, aes(quantile, est, ymin = est - 1.96*sd, ymax = est + 1.96*sd)) + 
    geom_pointrange()
```

TODO: what is `SingVarRiskSummaries` for; interpretation.

```{r}
risks.singvar <- SingVarRiskSummaries(fit = fitkm, y = y, Z = Z, X = X, 
                                      qs.diff = c(0.25, 0.75), 
                                      q.fixed = c(0.25, 0.50, 0.75),
                                      method = "exact")
risks.singvar
ggplot(risks.singvar, aes(variable, est, ymin = est - 1.96*sd, 
                          ymax = est + 1.96*sd, col = q.fixed)) + 
    geom_pointrange(position = position_dodge(width = 0.75)) + 
  coord_flip()
```

TODO: what is `SingVarRiskSummaries` for; interpretation.

```{r}
risks.int <- SingVarIntSummaries(fit = fitkm, y = y, Z = Z, X = X, 
                                 qs.diff = c(0.25, 0.75), 
                                 qs.fixed = c(0.25, 0.75),
                                 method = "exact")
risks.int
```

### Model Prediction

`SamplePred`

Use this function to provide example. Refer to the previous tutorial for this chapter and interpret it.

### MCMC Diagnostics

Below, we can visualize the general trace of the Markov Chain Monte Carlo method for the samples. This graphs the $\beta$ parameter used in the BKMR model.

```{r}
TracePlot(fit = fitkm, par = "beta")
```

Examining the traceplot, it appears the chains generally show no noticeable patterns across iterations (no apparent wandering trends). Additionally, the samples thoroughly span over a consistent range of parameter values. Therefore, our model has convergence and overall favorable mixing.

## Miscellaneous

### Hierarchical Variable Selection

One of the appeals of BKMR regression is the ability to use hierarchical variable selection. This method assesses which variables are critical to the model by using grouping. Primarily, the chemical exposures are grouped, often by their relationships with other exposures or by using prior knowledge. Each exposure in the Z matrix can be assigned to a group by using `groups = c(1,1,2,2)` where the first two variables in Z are assigned to group 1, while the remaining variables are placed in group 2. If the variables are not manually grouped, there is an option for component wise selection where variables are evaluated on an individual basis (`varsel = TRUE`). An additional chemical exposure was included in this example to demonstrate correlation between exposures for grouping purposes. Both methods are demonstrated below:

```{r}
nhanes_URX_new<-nhanes1518 |>
  select(BMXBMI,RIDAGEYR, URXMBP, URXMIB, URXMC1, URXMEP, URXMZP, URXECP, URXMHH, URXMOH, URXMHP)|>
  na.omit()|>
  mutate(BMXBMI = log(BMXBMI))

```

To understand which variables should be grouped, a correlation matrix is utilized to examine each variable's relationship:

```{r}
corr_mat=cor(nhanes_URX_new,method="s")
corrplot(corr_mat, # colorful number
         addCoef.col = 1,    # Change font size of correlation coefficients
         number.cex = 0.5) 
```

Based on the correlations, we place the non-correlated variables in group 1: URXMBP, URXMIB, URXMC1, URXMEP, URXMZP.

Highly correlated variables are included in group 2: URXECP, URXMHH, URXMOH, URXMHP.

```{r}
y_final<-nhanes_URX_new$BMXBMI # bmi
Z_final <- nhanes_URX_new |> select(-BMXBMI, -RIDAGEYR) # chemical mixtures
X_final <- nhanes_URX_new |> select(RIDAGEYR)

```

```{r}

fitkm_corr <- kmbayes(y = y_final, Z = Z_final, X = X_final, iter = 10, varsel = TRUE, verbose = FALSE)
fitkm_hier <- kmbayes(y = y_final, Z = Z_final, X = X_final, iter = 10, varsel = TRUE, 
                      groups = c(#NEED TO CHANGE), verbose = FALSE)
# group 1: URXMBP, URXMIB, URXMC1, URXMEP, URXMZP
# group 2: URXECP, URXMHH, URXMOH, URXMHP
```

We can contrast the posterior inclusion probabilities from the two models:

Where the exposures were grouped:

```{r}
ExtractPIPs(fitkm_corr)
```

\
For grouped exposures:

```{r}
ExtractPIPs(fitkm_hier)

```

### Changing Tunning Parameters or Prior Distribution

TODO: use two paragraphs, one for tunning parameters (pick the important ones), the other for prior (provide the default prior). and refer to the [github page](https://jenfb.github.io/bkmr/overview.html#changing_the_tuning_parameters_for_fitting_the_algorithm)

Another feature of the BKMR package is that tuning parameters for the fitting algorithm can be manually adjusted. The model uses the Markov chain Monte Carlo method and also uses Gibbs steps for updating the parameters. The parameters can be accessed through the `control.params` argument. In the list of tuning methods, parameters including $\lambda$ (accessed by `lambda.jump`) can be adjusted which is the standard deviaion of the proposed distribution ($\lambda = \tau / \sigma^2$) where $\tau$ is the variance of the kernel matrix, adjusting the smoothness of the exposure-response. Additional adjustments include the standard deviation of the proposal distribution which can adjusted by `r.jump`. Parameters for the Monte Carlo Method can also be adjusted dependent on if the variable is not included and then included in the model or if the variable remains in the proposed model. If the exposure is initially not included to included in the model, the standard deviation can be specified by `r.jump1`. If the variable stays in the proposed model, the standard deviation is accessed through `r.jump2`. Finally, the mean can be modified if the variable is not included to included through `r.muprop`.

In addition to tuning parameters, the the prior distribution can be adjusted. The prior distribution can be specified by `r.prior` with options including include “gamma”, “invunif”, and “unif”. The mean and standard deviation for a gamma prior can be adjusted by `mu.lambda` and `sigma.lambda`. Additional adjustments for the gamma prior include $\sigma^2$ altered through `a.sigsq`, `b.sigsq`. For a beta prior, the shape parameters $\pi$ can be manipulated through `a.p0`, `b.p0` and should be used when `varsel = TRUE`.

\*\* DELETE TABLE BELOW

| Parameter                                              | Parameter Call              | Uses                                       | Description/Notes                                                                  |
|--------------------------------------------------------|-----------------------------|--------------------------------------------|------------------------------------------------------------------------------------|
| $$                                                     
                \sigma^2                                 
                $$                                       | `a.sigsq`, `b.sigsq`        | Should be used with gaussian distributions | Described as the "shape/rate" of a gamma prior                                     |
| $$                                                     
                \lambda                                  
                $$                                       | `mu.lambda`, `sigma.lambda` |                                            | For gamma priors, it is mean and standard deviation                                |
| $$                                                     
                r_m                                      
                $$                                       | `r.prior`                   | Can be used with all models                | Specifies the prior distributions - options include “gamma”, “invunif”, and “unif” |
| $$                                                     
                \pi                                      
                $$                                       | `a.p0`, `b.p0`              | Should be used when `varsel = TRUE`        |                                                                                    |

Additional options can be found at the following link:

<https://jenfb.github.io/bkmr/overview.html>

## Discussion

TODO: since this is the last tutorial, compare this to the previous methods. Pros and Cons.

## References

<https://jenfb.github.io/bkmr/overview.html>
